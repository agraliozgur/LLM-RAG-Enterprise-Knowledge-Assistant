{"unique_id": "f86a1e14-a6e0-4d26-8e0f-4cefe336431a", "file_name": "CrateDB Blog _ Building AI Knowledge Assistants for Enterprise PDFs_ A Strategic Approach.pdf", "file_type": ".pdf", "chunk_id": 0, "chunk_text": "1/22/25, 10:22 PM CrateDB Blog | Building AI Knowledge Assistants for Enterprise PDFs: A Strategic Approach Live Stream on Jan 23rd: Unlocking Real Time Insights in the Renewable Energy Sector with CrateDB Register now Log In Start free Blog Building AI Knowledge Assistants for Enterprise PDFs: A Strategic Approach 2025-01-15 by Wierd van der Haar,4 minute read CHATBOT In today’s increasingly data-driven world, many organizations are sitting on mountains of information locked away in PDFs. Whether it’s business reports, regulatory documents, user manuals, or research papers, the ability to extract and utilize insights from these documents is becoming essential. The current platforms, like SharePoint, for example—do a pretty good job when it comes to text searches, but searching for images, or even within images, let alone performing truly semantic searches, is not possible. RAG, short for Retrieval Augmented Generation, is a framework designed for large language models (LLMs) to enhance their ability to access relevant, up-to-date, and context-specific information by seamlessly combining retrieval and generation capabilities. https://cratedb.com/blog/building-ai-knowledge-assistants-for-enterprise-pdfs 1/7 1/22/25, 10:22 PM CrateDB Blog | Building AI Knowledge Assistants for Enterprise PDFs: A Strategic Approach Live Stream on Jan 23rd: Unlocking Real Time Insights in the Renewable Energy Sector with CrateDB Register now That’s where AI Knowledge Assistants come in. At the core, these assistants are powered by a RAG pipeline, which efficiently processes and interprets both text and visual data and then integrates these insights into powerful Large Language Models (LLMs). This combination not only improves the accuracy of generated answers but also ensures that the answers remain grounded in the actual source material. This flow ensures that the AI Knowledge Assistant references ground-truth data from enterprise PDFs, yielding answers grounded in actual content rather than relying solely on a model’s internal parameters. Understanding the RAG Pipeline Retrieval Augmented Generation (RAG) pipelines are a crucial component of generative AI, enhancing a model's ability to generate accurate and contextually relevant content. RAG pipelines operate through a streamlined process involving data preparation, data retrieval, and response generation. Phase 1: Data Preparation https://cratedb.com/blog/building-ai-knowledge-assistants-for-enterprise-pdfs 2/7 1/22/25, 10:22 PM CrateDB Blog | Building AI Knowledge Assistants for Enterprise PDFs: A Strategic Approach During the data preparation phase, raw data, such as text, audio, etc., is extracted and Live Stream on Jan 23rd: Unlocking Real Time Insights in the Renewable Energy Sector with divided into smaller chunks. These chunks are then translated into embeddings and stored in a vector database. It is important to store the chunks and their metadata together with CrateDB the embeddings to reference back to the actual source of information in the retrieval phase. Register now Phase 2: Data Retrieval & Augmentation 1. Retrieval Component This component manages the retrieval of information from the knowledge base, where domain-specific data is stored in the format of vector embeddings. For example, when a user asks a question, the system creates an embedding of that query and searches for the most similar content in the vector database. 2. Augmentation Component This component enriches the quality"}
{"unique_id": "f86a1e14-a6e0-4d26-8e0f-4cefe336431a", "file_name": "CrateDB Blog _ Building AI Knowledge Assistants for Enterprise PDFs_ A Strategic Approach.pdf", "file_type": ".pdf", "chunk_id": 1, "chunk_text": "from the knowledge base, where domain-specific data is stored in the format of vector embeddings. For example, when a user asks a question, the system creates an embedding of that query and searches for the most similar content in the vector database. 2. Augmentation Component This component enriches the quality of the prompt by integrating context into the original user query. Essentially, the system augments the user’s question with the relevant information retrieved by the retrieval component, ensuring the Large Language Model (LLM) has direct access to domain-specific knowledge. Phase 3: Response Generation This is the component that generates the final output or answer based on the augmented prompt. Typically, Large Language Models (LLMs) are used for response generation because they have been trained on large amounts of text, enabling them to produce coherent and contextually relevant answers. https://cratedb.com/blog/building-ai-knowledge-assistants-for-enterprise-pdfs 3/7 1/22/25, 10:22 PM CrateDB Blog | Building AI Knowledge Assistants for Enterprise PDFs: A Strategic Approach While this is a simplified representation of the process, the real-world implementation Live Stream on Jan 23rd: Unlocking Real Time Insights in the Renewable Energy Sector with involves more intricate steps. Questions such as how to properly chunk and extract information from sources like PDF files or documentation and how to define and measure CrateDB relevance for re-ranking results are all part of broader considerations. Register now Why Organizations Are Building AI Knowledge Assistants 1. Unlocking Unstructured Data Most enterprise knowledge is still locked in PDFs, PowerPoints, and other unstructured formats. Transforming these documents into a form that’s directly usable by advanced AI models allows organizations to turn passive text into living knowledge. 2. Enhancing Decision-Making Executives and managers can query large sets of documents for data-driven decisions without having to manually sift through hundreds of files. This retrieval-based approach speeds up research, compliance checks, and other critical business processes. 3. Improving Customer Support and Self- Service A well-implemented RAG pipeline can power chatbots and automated helpdesks that understand customer queries and retrieve the most relevant passages from product manuals, FAQ documents, or internal wikis—all in real-time. 4. Streamlining Knowledge Management Once data is chunked, embedded, and stored, the foundation is laid for continuous learning and future expansions. Teams can build additional features—like advanced question- answering or recommendation systems—on top of the same pipeline. The Business Value of AI Knowledge Assistants https://cratedb.com/blog/building-ai-knowledge-assistants-for-enterprise-pdfs 4/7 1/22/25, 10:22 PM CrateDB Blog | Building AI Knowledge Assistants for Enterprise PDFs: A Strategic Approach 1. Reduced Costs Live Stream on Jan 23rd: Unlocking Real Time Insights in the Renewable Energy Sector with CrateDB Less human effort spent searching through documents. Lower costs from wasted time or duplicated efforts in multiple departments. Register now 2. Better Compliance and Risk Management Quickly surface relevant passages from regulatory and compliance PDFs. Avoid missing critical updates by maintaining real-time, AI-driven searches. 3. Accelerated Innovation Data-driven insights from up-to-date, relevant chunks of information. Rapid prototyping and iterative improvements driven by immediate feedback. 4. Competitive Differentiation Offering new features like intelligent document navigation or AI-driven"}
{"unique_id": "f86a1e14-a6e0-4d26-8e0f-4cefe336431a", "file_name": "CrateDB Blog _ Building AI Knowledge Assistants for Enterprise PDFs_ A Strategic Approach.pdf", "file_type": ".pdf", "chunk_id": 2, "chunk_text": "Quickly surface relevant passages from regulatory and compliance PDFs. Avoid missing critical updates by maintaining real-time, AI-driven searches. 3. Accelerated Innovation Data-driven insights from up-to-date, relevant chunks of information. Rapid prototyping and iterative improvements driven by immediate feedback. 4. Competitive Differentiation Offering new features like intelligent document navigation or AI-driven analytics. Building brand loyalty through smarter, more efficient user experiences. What You Will Learn in the Next Blog Posts Core Techniques Powering Enterprise Knowledge Assistants: Building a RAG pipeline for enterprise PDFs requires a thoughtful approach that balances business goals, technical rigor, and scalability. From extracting PDFs (including images and OCR) to chunking for better context, from embedding vectors to choosing a powerful multi-model database for storage, each step is crucial to overall performance and accuracy. Designing the Consumption Layer for Enterprise Knowledge Assistants: On the consumption side, selecting the right LLM or combination of models, addressing security concerns, and optimizing resource usage are essential to ensure you meet enterprise requirements. Step by Step Guide to Building a PDF Knowledge Assistant: Learn to build a production- ready PDF Knowledge Assistant with structured testing, data compliance, and robust monitoring for optimal performance and reliability. Making a Production-Ready AI Knowledge Assistant: Finally, adopting a structured testing framework helps validate your RAG pipeline and paves the way for consistent improvements, https://cratedb.com/blog/building-ai-knowledge-assistants-for-enterprise-pdfs 5/7 1/22/25, 10:22 PM CrateDB Blog | Building AI Knowledge Assistants for Enterprise PDFs: A Strategic Approach whether in chunking strategies, retrieval methods, or LLM fine-tuning. Live Stream on Jan 23rd: Unlocking Real Time Insights in the Renewable Energy Sector with RAG is not merely a technology stack—it’s a strategic lever that organizations can use to CrateDB unlock the full potential of their unstructured data. By investing in robust extraction, flexible data management, optimized retrieval, anRde cgoisntteinr unoouws testing, you can create intelligent, context-rich applications that put your PDF archives at the heart of innovation and decision- making. *** Continue reading: Core Techniques Powering Enterprise Knowledge Assistants Share Related Posts Core Techniques Powering Designing the Consumption Layer Enterprise Knowledge Assistants for Enterprise Knowledge 2025-01-15 Assistants To harness the potential of RAG, 2025-01-15 organizations need to master a few crucial Once your documents are processed (text building blocks. ... is chunked, embedded, and stored) — read \"Core techniques in an Enterprise READ MORE Knowledge Assistant\" — , you’re ready to answer user queries in real time. This stage... https://cratedb.com/blog/building-ai-knowledge-assistants-for-enterprise-pdfs 6/7 1/22/25, 10:22 PM CrateDB Blog | Building AI Knowledge Assistants for Enterprise PDFs: A Strategic Approach READ MORE Live Stream on Jan 23rd: Unlocking Real Time Insights in the Renewable Energy Sector with CrateDB Register now Step by Step Guide to Building a PDF Knowledge Assistant 2025-01-15 This guide outlines how to build a PDF Knowledge Assistant, covering: Setting up a project folder. Installing dependencies. Using two Python scripts (one for extracting data from PDFs, and one for cr... READ MORE Company Ecosystem Contact © 2024 CrateDB. All rights reserved. Legal | Privacy Policy | Imprint https://cratedb.com/blog/building-ai-knowledge-assistants-for-enterprise-pdfs 7/7"}
{"unique_id": "f86a1e14-a6e0-4d26-8e0f-4cefe336431a", "file_name": "CrateDB Blog _ Building AI Knowledge Assistants for Enterprise PDFs_ A Strategic Approach.pdf", "file_type": ".pdf", "chunk_id": 3, "chunk_text": "a PDF Knowledge Assistant, covering: Setting up a project folder. Installing dependencies. Using two Python scripts (one for extracting data from PDFs, and one for cr... READ MORE Company Ecosystem Contact © 2024 CrateDB. All rights reserved. Legal | Privacy Policy | Imprint https://cratedb.com/blog/building-ai-knowledge-assistants-for-enterprise-pdfs 7/7"}
{"unique_id": "e5938852-8f35-4e52-b9f6-f73390fe85db", "file_name": "The Ultimate Guide to Enterprise Knowledge Management [2024] _ Atomicwork.pdf", "file_type": ".pdf", "chunk_id": 0, "chunk_text": "1/22/25, 10:23 PM The Ultimate Guide to Enterprise Knowledge Management [2024] | Atomicwork Just launched: State of AI in IT 2025 report, partnering with ITIL’s parent company PeopleCert and ITSM.tools. Get your copy now -> ESM / The Ultimate Guide To Enterprise Knowledge Management In 2024 The ultimate guide to enterprise knowledge management in 2024 McKinsey reports that employees searching for internal data waste an average of 9.3 hours, or about 20% of their weekly working hours. This is equivalent to employing one dedicated employee whose only job is assisting different teams in https://www.atomicwork.com/esm/enterprise-knowledge-management-guide 1/28 1/22/25, 10:23 PM The Ultimate Guide to Enterprise Knowledge Management [2024] | Atomicwork providing the information required to generate productive output. And that’s not feasible for many enterprises. Most operate across different locations, and hiring one dedicated resource for information search at every location is too much of an overhead cost. This kind of setup only encourages the siloed structure that most enterprises are trying to lift off. But then, how do you ensure that the least time is devoted to finding information and solutions to problems that happen frequently or have happened in the past? Addressing inefficiencies like this requires a more structured approach to information handling, and this is where Enterprise Knowledge Management (EKM) comes in. EKM systems enable faster decision-making and increased productivity by creating a single source of truth for all scattered information. This guide will discuss enterprise knowledge management and its best practices in detail. Let’s get started. https://www.atomicwork.com/esm/enterprise-knowledge-management-guide 2/28 1/22/25, 10:23 PM The Ultimate Guide to Enterprise Knowledge Management [2024] | Atomicwork What is enterprise knowledge management? Enterprise Knowledge Management, or enterprise information management (EIM), is the systematic process of capturing, organizing, and leveraging an organization's data using advanced technology to enhance its strategic capabilities. It goes beyond simple information cataloging, focusing on the intelligent curation and deployment of knowledge assets to drive innovation, streamline decision- making, and facilitate cross- functional collaboration. A knowledge management system helps you find information and answers to your questions/ issues. It eliminates the need for a dedicated team to help find answers to already existing solutions. For example, you can quickly find answers to questions about company policies, like the leave policy or accessing the company VPN. https://www.atomicwork.com/esm/enterprise-knowledge-management-guide 3/28 1/22/25, 10:23 PM The Ultimate Guide to Enterprise Knowledge Management [2024] | Atomicwork With tools like Atomicwork, this becomes easier as the AI Assistant, Atom, understands your message and its context to provide the best answer and the source. For example, you can ask Atom questions like: How do I set up my email on my phone? What should I do if I receive a phishing email? When is the next payroll date? How do I apply for parental leave? How do I access my work files from my home? How has enterprise information management evolved? EIM has evolved significantly over the years, becoming more agile and user-friendly. Initially, organizations heavily relied on human agents or self-service portals that allowed employees access to information."}
{"unique_id": "e5938852-8f35-4e52-b9f6-f73390fe85db", "file_name": "The Ultimate Guide to Enterprise Knowledge Management [2024] _ Atomicwork.pdf", "file_type": ".pdf", "chunk_id": 1, "chunk_text": "do I apply for parental leave? How do I access my work files from my home? How has enterprise information management evolved? EIM has evolved significantly over the years, becoming more agile and user-friendly. Initially, organizations heavily relied on human agents or self-service portals that allowed employees access to information. The result was human agents overflowing with support requests they could barely handle. https://www.atomicwork.com/esm/enterprise-knowledge-management-guide 4/28 1/22/25, 10:23 PM The Ultimate Guide to Enterprise Knowledge Management [2024] | Atomicwork Alternatively, self-help portals were difficult to use and remained underutilized in most enterprises. They also lacked proper integration with daily operations and workflows and provided generic responses, limiting the effectiveness of early EIM systems. No wonder all these led to employees resorting to manual processes for information. As technology advanced, we witnessed the rise of collaboration platforms like Slack and MS Teams, which integrated EIM seamlessly into our daily operations. These platforms deliver enterprise information directly within the platform, empowering employees not to switch between multiple tools at the same time. In the last few years, this efficiency has been further enhanced by integrating AI-powered assistants with such collaboration platforms, leveraging LLMs, and offering more natural and contextual responses, saving valuable hours locating critical data or information. https://www.atomicwork.com/esm/enterprise-knowledge-management-guide 5/28 1/22/25, 10:23 PM The Ultimate Guide to Enterprise Knowledge Management [2024] | Atomicwork Why is managing enterprise knowledge important? Managing enterprise knowledge helps organizations and employees to: 1. Faster responses for end- users: With a robust knowledge management system, end-users can self-serve information and don't have to wait for their queries to be answered. This leads to quicker problem resolution and improved user satisfaction. 2. Reduced team workload: Service agents are not bogged down with repetitive queries as requests are deflected from the service desk with a rich knowledge management system. This allows support teams to focus on more complex issues and strategic initiatives. For example, Ammex Corp, a leading safety gloves distributor, experienced significant improvements after implementing an AI-driven knowledge management system. They were able to achieve a query deflection https://www.atomicwork.com/esm/enterprise-knowledge-management-guide 6/28 1/22/25, 10:23 PM The Ultimate Guide to Enterprise Knowledge Management [2024] | Atomicwork rate of 65% with our AI assistant, Atom. This improvement allowed Ammex to maintain its IT service team without adding any headcount for six months despite the company's growth. The ROI on deploying Atom across our teams has been incredible. Unlike Jira Service Management, Atom allowed us to maintain our IT service team without adding a single headcount in six months. It handles simple queries that used to interrupt our Finance team, and it provides our CEO with real-time updates on shipments and orders - questions that would normally require a phone call or an https://www.atomicwork.com/esm/enterprise-knowledge-management-guide 7/28 1/22/25, 10:23 PM The Ultimate Guide to Enterprise Knowledge Management [2024] | Atomicwork email or a meeting, disrupting someone’s day - Chad Ghosn, Ammex’s CIO and CTO Read the complete case study here. 3. Manage support costs: Businesses can control support costs with a lean support team and an effective"}
{"unique_id": "e5938852-8f35-4e52-b9f6-f73390fe85db", "file_name": "The Ultimate Guide to Enterprise Knowledge Management [2024] _ Atomicwork.pdf", "file_type": ".pdf", "chunk_id": 2, "chunk_text": "1/22/25, 10:23 PM The Ultimate Guide to Enterprise Knowledge Management [2024] | Atomicwork email or a meeting, disrupting someone’s day - Chad Ghosn, Ammex’s CIO and CTO Read the complete case study here. 3. Manage support costs: Businesses can control support costs with a lean support team and an effective self-serve system. Organizations can allocate resources more efficiently and reduce overall operational costs by reducing the need for large support teams to handle routine queries. 4. Improved productivity and efficiency: A well-managed system streamlines processes and reduces redundancy, helping improve decision-making across departments. Companies like Siemens implemented knowledge- sharing platforms to streamline the deployment of new technologies and optimize processes, improving operational efficiency. 5. Improves cross-department collaboration: Enterprise information management streamlines open communications https://www.atomicwork.com/esm/enterprise-knowledge-management-guide 8/28 1/22/25, 10:23 PM The Ultimate Guide to Enterprise Knowledge Management [2024] | Atomicwork and collaboration. It helps employees be more productive by quickly finding the answer to their queries. Google uses open communication and data-driven decision-making to encourage collaboration and ongoing learning, leading to groundbreaking innovations. 6. Standardize knowledge discovery and handling: When you use a knowledge management system, you set standardized methods and processes for capturing data, storing it, and disseminating it. This helps create a structured format and consistency in knowledge assets, making it easier for employees to consume the information. 7. Helps mitigate loss of information: Without centralized knowledge management, information retrieval depends on individuals. Therefore, there is a risk of losing critical information after an employee leaves. Knowledge management mitigates this risk by creating an automated system that pulls information from various sources, updates it, keeps it safe, and makes it usable. The stages of enterprise knowledge management https://www.atomicwork.com/esm/enterprise-knowledge-management-guide 9/28 1/22/25, 10:23 PM The Ultimate Guide to Enterprise Knowledge Management [2024] | Atomicwork Managing enterprise knowledge is a multi-stage process that requires careful planning and implementation. Let us break down the critical stages: Stage 1: Knowledge capture The first stage covers identifying Product Solutions Pricing Sign in In This Guide: and collecting useful data and information from other sources in Resources Company Schedule demo the enterprise. These sources involve communication channels Share Article like Slack and Teams, asset management platforms like SharePoint and Google Drive, HR and payroll software, etc. Stage 2: Knowledge storage After capturing, the knowledge data is organized and stored for easy retrieval. This involves systematically arranging and indexing the data, using knowledge management systems, databases, and repositories for intuitive searches. Stage 3: Knowledge sharing If knowledge is not available or remains hidden, it loses its value. To help employees discover the right information at the right time, use systems designed to provide easy access to knowledge. These systems tailor the information to the https://www.atomicwork.com/esm/enterprise-knowledge-management-guide 10/28 1/22/25, 10:23 PM The Ultimate Guide to Enterprise Knowledge Management [2024] | Atomicwork user’s specific needs and quickly retrieve it, maximizing the adoption and ROI of EKM. How to manage enterprise information effectively? A solid information management strategy allows employees to easily find the needed resources, streamlines workflows, and minimizes inefficiencies. Organizations"}
{"unique_id": "e5938852-8f35-4e52-b9f6-f73390fe85db", "file_name": "The Ultimate Guide to Enterprise Knowledge Management [2024] _ Atomicwork.pdf", "file_type": ".pdf", "chunk_id": 3, "chunk_text": "PM The Ultimate Guide to Enterprise Knowledge Management [2024] | Atomicwork user’s specific needs and quickly retrieve it, maximizing the adoption and ROI of EKM. How to manage enterprise information effectively? A solid information management strategy allows employees to easily find the needed resources, streamlines workflows, and minimizes inefficiencies. Organizations must have a well- structured system that facilitates smooth information flow and boosts overall productivity. To accomplish this, organizations must utilize advanced tools specifically designed for enterprise knowledge management. Atomicwork simplifies information handling by providing a centralized hub, automating routine tasks, and ensuring secure data management. It pulls information from trusted public sources for common IT questions, like troubleshooting steps or how-tos for tools in your enterprise stack. https://www.atomicwork.com/esm/enterprise-knowledge-management-guide 11/28 1/22/25, 10:23 PM The Ultimate Guide to Enterprise Knowledge Management [2024] | Atomicwork For example, if you want to know why Outlook isn’t syncing between your phone and desktop, need help installing Zoom on your laptop, or have encountered error codes in Salesforce, just ask the Atomicwork assistant. It will give you a concise summary of the tool’s support site. But how can you efficiently manage your enterprise’s data? Here’s a step-by-step guide: 1. Set up your communication channels Define how your teams, such as Slack or MS Teams, will interact with the information in the system. Ensure that your key communication platforms are connected. This makes finding information easier for employees as they can directly access information from their familiar channels. For teams relying on email communication, enable email forwarding. With Atomicwork integration, you get a conversational AI assistant, Atom, that helps you find answers to your questions and resolve issues yourself. You can interact by @mentioning Atom in a channel or through DMs. https://www.atomicwork.com/esm/enterprise-knowledge-management-guide 12/28 1/22/25, 10:23 PM The Ultimate Guide to Enterprise Knowledge Management [2024] | Atomicwork 2. Organize different workspaces for every department Different departments in your organization have unique information management needs. For instance: Your HR department may need to maintain and make information on leave policies and nominee processes easily accessible The IT team might want to own a repository of troubleshooting guides for common device issues The Finance department could require a centralized location for expense policies and reimbursement procedures To address these needs, it's crucial to organize your teams and knowledge sources to ensure employees have access to the right information from the right team. This is where the concept of workspaces comes in. With Atomicwork, you can segment and set up dedicated 'workspaces' for each team or department. These workspaces allow you to: Create separate knowledge hubs for HR, IT, finance, and other departments https://www.atomicwork.com/esm/enterprise-knowledge-management-guide 13/28 1/22/25, 10:23 PM The Ultimate Guide to Enterprise Knowledge Management [2024] | Atomicwork Customize each workspace with its own set of knowledge sources and information Ensure that employees can easily find department-specific information without wading through irrelevant data For example, you can set up an HR workspace with all HR-related policies and procedures, an IT workspace with technical guides and troubleshooting information, and a finance"}
{"unique_id": "e5938852-8f35-4e52-b9f6-f73390fe85db", "file_name": "The Ultimate Guide to Enterprise Knowledge Management [2024] _ Atomicwork.pdf", "file_type": ".pdf", "chunk_id": 4, "chunk_text": "workspace with its own set of knowledge sources and information Ensure that employees can easily find department-specific information without wading through irrelevant data For example, you can set up an HR workspace with all HR-related policies and procedures, an IT workspace with technical guides and troubleshooting information, and a finance workspace with budgeting tools and expense guidelines. This segmentation helps streamline information access and maintains content relevance for each department. 3. Connect your knowledge sources Finding accurate answers requires the AI to be connected with the right knowledge sources and learn from them continuously. It learns from: Conversations in the Slack/Microsoft channels you’ve added The documents you upload, like PDFs, CSV files, etc., and the URLs you provide. Notion pages and SharePoint documents are available if you https://www.atomicwork.com/esm/enterprise-knowledge-management-guide 14/28 1/22/25, 10:23 PM The Ultimate Guide to Enterprise Knowledge Management [2024] | Atomicwork connect your Notion/SharePoint account to Atomicwork. FAQs which can be saved as verified answers (we’ll check this next) To do so, you must connect your workspace to various knowledge sources like SharePoint, Confluence, and Notion. This helps your AI assistant to pull knowledge directly from these platforms and keep them up-to-date. For example, you can link Confluence to the IT workspace if your IT department uses that for documentation. Upload all the documents relevant to each department, such as the employee handbook, PPT, or other documents. Add all the relevant URLs, such as the company’s VPN access guide, so that the database is updated frequently. Atomicwork lets you link various external platforms, upload documents, and add URLs directly. You can upload important documents with the following extensions: DOC/DOCX, https://www.atomicwork.com/esm/enterprise-knowledge-management-guide 15/28 1/22/25, 10:23 PM The Ultimate Guide to Enterprise Knowledge Management [2024] | Atomicwork PPT/PPTX, XLS/XLSX, PDF, ASPX, CSV, or TXT. 4. Organize by topics The best way to expedite the search is to categorize the knowledge in the workspaces you set up by adding relevant topics. This helps the AI assistant provide you with more accurate answers quickly by understanding the context of the queries. For example, topics for IT workspace could include software installation, password resets, and network issues. For HR, these can include leave policies, onboarding, and employee benefits. Pro-tip: Define the audience for each topic that you add. This will help the AI find answers to the document topic only for employees added to the segment. 5. Setting up verified answers As enterprise information management has evolved, we've seen a shift toward using AI assistants and Large Language Models (LLMs) for knowledge retrieval. While these AI systems https://www.atomicwork.com/esm/enterprise-knowledge-management-guide 16/28 1/22/25, 10:23 PM The Ultimate Guide to Enterprise Knowledge Management [2024] | Atomicwork can generate answers on their own, there's a more efficient approach for handling frequently asked questions: verified answers. Verified answers are pre-approved responses to common queries reviewed and validated by subject matter experts within your organization. They offer several key advantages: Providing a single, correct answer to specific questions ensures that all employees receive the same accurate information every time When an AI"}
{"unique_id": "e5938852-8f35-4e52-b9f6-f73390fe85db", "file_name": "The Ultimate Guide to Enterprise Knowledge Management [2024] _ Atomicwork.pdf", "file_type": ".pdf", "chunk_id": 5, "chunk_text": "frequently asked questions: verified answers. Verified answers are pre-approved responses to common queries reviewed and validated by subject matter experts within your organization. They offer several key advantages: Providing a single, correct answer to specific questions ensures that all employees receive the same accurate information every time When an AI assistant encounters a question with a verified answer, it doesn't need to generate a response using the LLM. Instead, it can directly fetch and deliver the pre-approved answer. This bypasses the need for pre- processing and post-processing, significantly reducing computational demands Because verified answers don't require real-time generation, they can be delivered almost instantaneously, improving user experience Quality Control: Subject matter experts can review and approve these answers, ensuring the information provided is always correct and up-to-date https://www.atomicwork.com/esm/enterprise-knowledge-management-guide 17/28 1/22/25, 10:23 PM The Ultimate Guide to Enterprise Knowledge Management [2024] | Atomicwork To implement verified answers effectively: Identify common questions across your organization Draft clear, concise answers to these questions Have subject matter experts review and approve these answers Input these verified answers into your knowledge management system For example, if someone asks, \"How do I connect to the company VPN?\" Instead of generating an answer each time or risking providing inconsistent information, your AI assistant can immediately provide the verified answer containing step-by-step instructions specific to your organization's process. Challenges in enterprise information management https://www.atomicwork.com/esm/enterprise-knowledge-management-guide 18/28 1/22/25, 10:23 PM The Ultimate Guide to Enterprise Knowledge Management [2024] | Atomicwork In the absence of a dedicated enterprise information management tool, enterprises often encounter any one or more of the following challenges: Lack of unified best practices leading to inefficiencies, poor data quality and security risks Automating data extraction from various structured and unstructured data sources is complex and requires advanced tools Information overload with the overwhelming volume of data which makes filtering valuable insights difficult, affecting decision-making Information silos where data is stored in isolated systems, blocking collaboration and leading to duplicated efforts Integration with incompatible legacy systems, making integration challenging and costly Maintaining compliance with evolving data privacy regulations, requiring strict management and documentation practices Ensuring employees have seamless access to relevant information across devices and systems https://www.atomicwork.com/esm/enterprise-knowledge-management-guide 19/28 1/22/25, 10:23 PM The Ultimate Guide to Enterprise Knowledge Management [2024] | Atomicwork Best practices for enhancing enterprise information management Here are a few best practices to set up your enterprise information management system from scratch: 1. Align EIM with an enterprise’s culture Create a culture of knowledge sharing and integrate a knowledge management system that streamlines the entire process for the following: Find the information you need instantly from your company's knowledge base, trusted public answers Keeps answers up-to-date from your public and standard channels Helps solve common problems by yourself with one-touch request and resolution skills Helps raise requests with your team or report incidents effortlessly Connects through multiple channels and gets updates on https://www.atomicwork.com/esm/enterprise-knowledge-management-guide 20/28 1/22/25, 10:23 PM The Ultimate Guide to Enterprise Knowledge Management [2024] | Atomicwork your requests 2. Simplify knowledge sharing With templates and"}
{"unique_id": "e5938852-8f35-4e52-b9f6-f73390fe85db", "file_name": "The Ultimate Guide to Enterprise Knowledge Management [2024] _ Atomicwork.pdf", "file_type": ".pdf", "chunk_id": 6, "chunk_text": "by yourself with one-touch request and resolution skills Helps raise requests with your team or report incidents effortlessly Connects through multiple channels and gets updates on https://www.atomicwork.com/esm/enterprise-knowledge-management-guide 20/28 1/22/25, 10:23 PM The Ultimate Guide to Enterprise Knowledge Management [2024] | Atomicwork your requests 2. Simplify knowledge sharing With templates and easy processes, employees can easily contribute knowledge. Tools like Atomicwork automate routine questions and workflows, easing processes, making knowledge quickly accessible, and allowing teams to spend time on more important work. 3. Assign a dedicated knowledge manager This enhances the content quality and makes alignment of KM initiatives with organizational goals easy. This dedicated manager will be responsible for: Overseeing the knowledge management plan Organizing, categorizing, and tagging information Conducting audits to evaluate the quality and relevance of existing knowledge and identifying gaps Implement access controls for sensitive data Strategize, monitor, and analyze usage and consumption patterns and improve the system https://www.atomicwork.com/esm/enterprise-knowledge-management-guide 21/28 1/22/25, 10:23 PM The Ultimate Guide to Enterprise Knowledge Management [2024] | Atomicwork 4. Measure the metrics for continuous improvement To ensure the success and continuous improvement of your Enterprise Information Management (EIM) system, it's crucial to track key metrics that reflect employee adoption and satisfaction. By monitoring these metrics, businesses can demonstrate the effectiveness of their EIM system and identify areas for enhancement. Key metrics to track include: Ticket deflection rates: This metric shows how effectively your EIM system reduces agent workload by enabling self-service. A high deflection rate indicates that employees are finding answers without creating support tickets Average response times: Faster response times generally correlate with higher employee satisfaction. Monitor how quickly employees receive answers to their queries through the EIM system First contact resolution rates: This metric measures how often employees' issues are resolved on their first interaction with the EIM system. Higher rates are directly tied to improved employee satisfaction https://www.atomicwork.com/esm/enterprise-knowledge-management-guide 22/28 1/22/25, 10:23 PM The Ultimate Guide to Enterprise Knowledge Management [2024] | Atomicwork User feedback: Atomicwork allows users to flag whether responses are helpful or unhelpful. This direct feedback is invaluable for improving the system Usage frequency: Track how often employees are using the EIM system. Increased usage often indicates growing trust and reliance on the system Top searched queries: Identifying frequently asked questions can help you prioritize content creation and updates 5. Implement AI guardrails and ethical guidelines Establishing clear guardrails and ethical guidelines is crucial when integrating AI assistants into your enterprise information management system. This ensures responsible AI use and protects your organization and employees. Key considerations include: Configure your AI to cite information sources and acknowledge AI-generated responses Implement robust permissions to prevent unauthorized access to https://www.atomicwork.com/esm/enterprise-knowledge-management-guide 23/28 1/22/25, 10:23 PM The Ultimate Guide to Enterprise Knowledge Management [2024] | Atomicwork sensitive information. For example, employees shouldn't be able to request colleagues' data Define off-limits topics for AI- generated responses, such as religion or politics Filter user input to protect AI models from harmful data. Continuously monitor AI- generated outputs for compliance with ethical guidelines and"}
{"unique_id": "e5938852-8f35-4e52-b9f6-f73390fe85db", "file_name": "The Ultimate Guide to Enterprise Knowledge Management [2024] _ Atomicwork.pdf", "file_type": ".pdf", "chunk_id": 7, "chunk_text": "Knowledge Management [2024] | Atomicwork sensitive information. For example, employees shouldn't be able to request colleagues' data Define off-limits topics for AI- generated responses, such as religion or politics Filter user input to protect AI models from harmful data. Continuously monitor AI- generated outputs for compliance with ethical guidelines and policies Maintain comprehensive logs of AI interactions and ensure traceable decision paths Implement a user feedback system and review AI performance to improve accuracy and relevance These guardrails align with responsible AI practices, such as those outlined in the TRUST (Transparent, Responsible, User- centric, Secure, and Traceable) framework. Implementing these measures allows you to leverage AI's power in enterprise information management while maintaining control and ensuring ethical use. Conclusion https://www.atomicwork.com/esm/enterprise-knowledge-management-guide 24/28 1/22/25, 10:23 PM The Ultimate Guide to Enterprise Knowledge Management [2024] | Atomicwork The time wasted searching for information is counterproductive and stifles innovation. Enterprise Knowledge Management changes this by centralizing data, breaking silos, and integrating knowledge into workflows, enabling faster, smarter decisions. Atomicworkaddresses these challenges with a comprehensive solution. It offers a centralized document hub that eliminates silos and makes essential information easily accessible. By automating data workflows, the platform enables employees to concentrate on more meaningful tasks, enhancing productivity. Its AI- powered contextual search allows quick access to accurate information while integrated collaboration tools foster seamless teamwork across various locations. Want to see Atomicwork in action? Book a demo! Frequently asked questions https://www.atomicwork.com/esm/enterprise-knowledge-management-guide 25/28 1/22/25, 10:23 PM The Ultimate Guide to Enterprise Knowledge Management [2024] | Atomicwork What is enterprise knowledge management? What is the role of a knowledge management system in the enterprise? How does AI help in enterprise information management? Does Atomicwork offer an enterprise knowledge management system? More resources on modern ITSM Building Atomicwork AI in IT AI in IT Embracing Responsible AI Leveraging AI workflows A CIO’s Guide: Practices with the TRUST for enterprise automation Understanding virtual Framework assistants, copilots, and AI AI workflows can help agents Unveiling our AI security and businesses break the constraints compliance framework that helps of traditional workflows that are Our break down of key AI CIOs and IT leaders to deliver rigid and siloed to deliver positive technologies to improve IT exceptional value with enterprise end-user experiences. support and agent productivity. https://www.atomicwork.com/esm/enterprise-knowledge-management-guide 26/28 1/22/25, 10:23 PM The Ultimate Guide to Enterprise Knowledge Management [2024] | Atomicwork AI while upholding ethical and security standards. Guide Guide Guide 15 Best enterprise Enterprise Workflow The ultimate guide to workflow management Automation: Benefits, Use enterprise service software for 2025 Cases, Challenges management (ESM) in 2025 Enterprise workflow management Automating common workflows software helps reduce redundant across your enterprise has several Discover the key benefits, use tasks at your organization across benefits. Read this guide to cases, challenges, and trends of departments. Here's our roundup discover the importance and top enterprise service management. of the top 15 tools you can scenarios you can pick for consider for enterprise workflow automation. automation. https://www.atomicwork.com/esm/enterprise-knowledge-management-guide 27/28 1/22/25, 10:23 PM The Ultimate Guide to Enterprise Knowledge"}
{"unique_id": "e5938852-8f35-4e52-b9f6-f73390fe85db", "file_name": "The Ultimate Guide to Enterprise Knowledge Management [2024] _ Atomicwork.pdf", "file_type": ".pdf", "chunk_id": 8, "chunk_text": "Read this guide to cases, challenges, and trends of departments. Here's our roundup discover the importance and top enterprise service management. of the top 15 tools you can scenarios you can pick for consider for enterprise workflow automation. automation. https://www.atomicwork.com/esm/enterprise-knowledge-management-guide 27/28 1/22/25, 10:23 PM The Ultimate Guide to Enterprise Knowledge Management [2024] | Atomicwork Product Features Solutions Industries Resources Overview Conversational Modern ITSM Healthcare Blog AI assistant software Pricing Manufacturing Podcast AI Agents IT workflow Features automation Retail Webinars OOTB Integrations automation IT knowledge Education management Security and Customizable View all Trending articles compliance workflows Employee self- industries -> service Help center Asset Modern ESM 101 management Automated Sign in employee onbo Ultimate guide to Incident arding ITIL V4 Release management For IT teams Getting started Status Request with AI in ITSM management For HR teams Modern guide to Change View all incident management solutions -> management Company AI Assistant for Employee self- About Agents service 101 Careers View all Compare Asset features -> management Press kit Atomicwork vs. guide 2024 ServiceNow Newsroom Atomicwork vs. Contact Integrations JSM Terms of Slack service Microsoft apps Privacy policy MS Teams Azure AD Intune View all integrations -> © Atomicwork Inc. https://www.atomicwork.com/esm/enterprise-knowledge-management-guide 28/28"}
{"unique_id": "d71911f7-8159-4ceb-9db0-7234a7fe80a8", "file_name": "CrateDB Blog _ Core Techniques Powering Enterprise Knowledge Assistants.pdf", "file_type": ".pdf", "chunk_id": 0, "chunk_text": "1/22/25, 10:21 PM CrateDB Blog | Core Techniques Powering Enterprise Knowledge Assistants Live Stream on Jan 23rd: Unlocking Real Time Insights in the Renewable Energy Sector with CrateDB Register now Log In Start free Blog Core Techniques Powering Enterprise Knowledge Assistants 2025-01-15 by Wierd van der Haar,5 minute read CHATBOT To harness the potential of RAG, organizations need to master a few crucial building blocks. *** This article is part of blog series. If you haven't read the previous article yet, be sure to check it out: Building AI Knowledge Assistants for Enterprise PDFs: A Strategic Approach 1. Extracting from PDFs Before you can feed your data into an RAG pipeline, you need to extract it from PDFs. This step sets the foundation for the entire workflow. The goal of your chatbot—whether it needs to present actual images, provide text-only responses, or generate image descriptions— Hi there! I am Goatie, the directly impacts how you extract and process each PDF. For instance, if your chatbot must CrateDB chat assistant. How can display or summarize images, you’ll need dedicated mechanisms to handle, store, and I help you? retrieve them; if you’re only interested in text, you can focus on raw text extraction and OCR. Text Extraction https://cratedb.com/blog/core-techniques-in-an-enterprise-knowledge-assistants 1/6 1/22/25, 10:21 PM CrateDB Blog | Core Techniques Powering Enterprise Knowledge Assistants Use libraries or services that identify text within PDFs. For straightforward text, standard Live Stream on Jan 23rd: Unlocking Real Time Insights in the Renewable Energy Sector with PDF parsing libraries work. However, be mindful of formatting, especially in scanned PDFs with no digital text layer. CrateDB Also consider headers and footers, whichR oefgtiesnte cro nnotwain valuable information like document titles, chapter names, page numbers, or dates. You may opt to remove them from the main body of text and store them separately as part of the document’s metadata. Image Detection Some PDFs include images or diagrams that may hold critical information. Identifying these images is essential if you need a fully comprehensive pipeline that can reference not just text but also visual elements. OCR (Optical Character Recognition) OCR transforms the scanned images of text into machine-readable text, thereby creating a “digital text layer” where none existed before. This ensures you can index, extract, and analyze the content just like any other text-based PDF. The process can be resource- intensive (often requiring GPUs), but it’s indispensable for processing large volumes of scanned documents. Table Extraction When PDFs contain data in tabular format, consider using specialized tools or libraries (e.g., Tabula, Camelot) to extract tables accurately. Tables often include important figures or text organized in rows and columns, which may otherwise be lost if parsed as standard text. Decide whether to keep the table structure (e.g., converting to CSV or HTML) or to summarize the data for downstream tasks like embedding or semantic search. Metadata Collection Don’t forget about titles, authors, creation dates, and other metadata. These details help with advanced filtering and can also influence the retrieval steps later. In some"}
{"unique_id": "d71911f7-8159-4ceb-9db0-7234a7fe80a8", "file_name": "CrateDB Blog _ Core Techniques Powering Enterprise Knowledge Assistants.pdf", "file_type": ".pdf", "chunk_id": 1, "chunk_text": "the table structure (e.g., converting to CSV or HTML) or to summarize the data for downstream tasks like embedding or semantic search. Metadata Collection Don’t forget about titles, authors, creation dates, and other metadata. These details help with advanced filtering and can also influence the retrieval steps later. In some cases, you might add header and footer data here if it provides contextual clues or helps distinguish versions of a document. 2. Chunking Extracted Data https://cratedb.com/blog/core-techniques-in-an-enterprise-knowledge-assistants 2/6 1/22/25, 10:21 PM CrateDB Blog | Core Techniques Powering Enterprise Knowledge Assistants Unlike plain search indexes, RAG pipelines often split documents into chunks—manageable Live Stream on Jan 23rd: Unlocking Real Time Insights in the Renewable Energy Sector with text segments used to create embeddings. The reason is simple: LLMs work better when prompts are concise, context-rich, and specific. CrateDB Fixed-Size Chunking (with overlap): StraRiegghitsfoterrw naordw approach where each chunk is a fixed number of tokens or characters, and adjacent chunks overlap slightly to retain context. Structure and Content-Aware Chunking: Considers sentences, paragraphs, sections, or chapters when chunking. Preserve logical boundaries, which can significantly improve retrieval quality. Document-Based Chunking: With this chunking method, you split a document based on its inherent structure. This approach respects the natural flow of the content but may not be as effective for documents that lack a clear structure. Hierarchical Chunking: Combines fixed-size and structure-awareness by chunking at multiple levels (document → chapter → paragraph) and linking them in a parent-child relationship. Semantic Chunking: The main idea is to group text segments with similar meaning. You create embeddings for each segment, then compare those embeddings to see which ones are most closely related. This approach keeps similar ideas together, preventing arbitrary splits that could harm retrieval quality. Agentic Chunking: This method empowers an LLM to dynamically decide how to split the text into chunks. We begin by extracting short, independent statements from the text and let an LLM agent determine if each statement should join an existing chunk or start a new one. Because the model understands context, it can produce more coherent chunks than fixed or structural methods. 3. Generating Embeddings Once you have your chunks, each chunk needs a vector representation (an embedding) that captures its semantic meaning. Choosing Embedding Models Security Considerations: The classification of your documents might prohibit the use of online LLMs, pushing you toward an on-premise or self-hosted model. If confidentiality is a priority, local deployment can ensure that no data leaves your environment. https://cratedb.com/blog/core-techniques-in-an-enterprise-knowledge-assistants 3/6 1/22/25, 10:21 PM CrateDB Blog | Core Techniques Powering Enterprise Knowledge Assistants Data Types (Text, Tables, Images, Multimodal): Live Stream on Jan 23rd: Unlocking Real Time Insights in the Renewable Energy Sector with Text: A text-based embedding model may be sufficient if you only have textual data. CrateDB Tables: For tabular data, you may need to transform the table into a more descriptive text format or use a specialized appRroeagcisht etor nporewserve row/column relationships. One strategy is to summarize tables first and then generate embeddings from those"}
{"unique_id": "d71911f7-8159-4ceb-9db0-7234a7fe80a8", "file_name": "CrateDB Blog _ Core Techniques Powering Enterprise Knowledge Assistants.pdf", "file_type": ".pdf", "chunk_id": 2, "chunk_text": "model may be sufficient if you only have textual data. CrateDB Tables: For tabular data, you may need to transform the table into a more descriptive text format or use a specialized appRroeagcisht etor nporewserve row/column relationships. One strategy is to summarize tables first and then generate embeddings from those summaries. Images: If your chatbot must search for images via text or by providing another image (“show me images similar to this”), you’ll need to generate embeddings for the images If you only need to display the original images without advanced search features, you may opt to store them directly in your database. Multimodal models (e.g., CLIP or GPT-4 Vision) can handle both text and images, enabling semantic search across different data types. Task Orientation: Think about the end goal—text-to-text, image-to-text, image-to- image, or table-based queries. Different scenarios benefit from specialized embedding models. Performance Considerations Hardware Requirements: Embedding models often require GPUs or other accelerators for efficient batch processing. Local vs. Cloud Deployment: Weigh the cost and convenience of a cloud solution against the benefits of total control and data sovereignty offered by an on-premises model. Image & Table Processing: Generating embeddings for images or large tables typically requires more compute resources and sometimes specialized libraries or frameworks. 4. Storing the Data The diversity of data and the sophistication of AI models demand a flexible, powerful, and nuanced approach to data management. As AI continues to penetrate various sectors, the need for databases that can adapt to complex data landscapes becomes paramount. The future of multi-model databases in AI shines—as an enabler of complex, context-rich, and real-time intelligent applications. In a RAG workflow, you need to store: https://cratedb.com/blog/core-techniques-in-an-enterprise-knowledge-assistants 4/6 1/22/25, 10:21 PM CrateDB Blog | Core Techniques Powering Enterprise Knowledge Assistants 1. Raw Text (and possibly images or OCR’d text) Live Stream on Jan 23rd: Unlocking Real Time Insights in the Renewable Energy Sector with 2. Embeddings (vectors) 3. Metadata (title, author, date, source) CrateDB A robust multi-model database that can hRaengdislete rre anlo-wtime ingestion of large datasets, manage high concurrency, and scale horizontally is a key piece of infrastructure. It should offer flexibility (for structured, unstructured, or semi-structured data), speed (sub-second queries on large datasets), and advanced search functionalities. *** Continue reading: Designing the Consumption Layer for Enterprise Knowledge Assistants Share Related Posts Building AI Knowledge Assistants for Enterprise PDFs: A Strategic Approach 2025-01-15 Step by Step Guide to Building a In today’s increasingly data-driven world, PDF Knowledge Assistant many organizations are sitting on 2025-01-15 mountains of information locked away in This guide outlines how to build a PDF PDFs. Whether it’s business reports, Knowledge Assistant, covering: Setting up regulatory documents, user manuals, or a project folder. Installing dependencies. researc... Using two Python scripts (one for READ MORE extracting data from PDFs, and one for cr... https://cratedb.com/blog/core-techniques-in-an-enterprise-knowledge-assistants 5/6 1/22/25, 10:21 PM CrateDB Blog | Core Techniques Powering Enterprise Knowledge Assistants READ MORE Live Stream on Jan 23rd: Unlocking Real Time Insights in the Renewable Energy Sector with CrateDB Register"}
{"unique_id": "d71911f7-8159-4ceb-9db0-7234a7fe80a8", "file_name": "CrateDB Blog _ Core Techniques Powering Enterprise Knowledge Assistants.pdf", "file_type": ".pdf", "chunk_id": 3, "chunk_text": "researc... Using two Python scripts (one for READ MORE extracting data from PDFs, and one for cr... https://cratedb.com/blog/core-techniques-in-an-enterprise-knowledge-assistants 5/6 1/22/25, 10:21 PM CrateDB Blog | Core Techniques Powering Enterprise Knowledge Assistants READ MORE Live Stream on Jan 23rd: Unlocking Real Time Insights in the Renewable Energy Sector with CrateDB Register now Designing the Consumption Layer for Enterprise Knowledge Assistants 2025-01-15 Once your documents are processed (text is chunked, embedded, and stored) — read \"Core techniques in an Enterprise Knowledge Assistant\" — , you’re ready to answer user queries in real time. This stage... READ MORE Company Ecosystem Contact © 2024 CrateDB. All rights reserved. Legal | Privacy Policy | Imprint https://cratedb.com/blog/core-techniques-in-an-enterprise-knowledge-assistants 6/6"}
{"unique_id": "046f2056-da95-4fea-b98d-d489b8d3cabf", "file_name": "llm_blog_image.png", "file_type": ".png", "chunk_id": 0, "chunk_text": "Workativ's Hybrid NLU * İn chatbot development, its important to achieve the right balance between accuracy, scalability, and cost-effectiveness. * Workativ Hybrid NLU utilizes the generative capabilities of a Large Language Model (LLM) and the intent detection capabilities of Workativ Al to provide a powerful and versatile hybrid solution for enterprise virtual agents. rl g e na yz See e ö —— © md Engine Urhan intent Medium Erpisimabey Ranker & Resolver \"İş üm yi Maas Len üfler Medium Control Medium Cost » © z FB Knomledgbase (SharePoie, Sinek, TSM, POFS, FAS, CRM, HRMS)"}
{"unique_id": "54b3b3f3-a8cd-4a3d-a52e-2ed72efa0456", "file_name": "task_management_automation.png", "file_type": ".png", "chunk_id": 0, "chunk_text": "HOME < SOLUTIONS < ENTERPRİSE KNOWLEDGE ASSISTANT Selected Use Cases - Task Management Automation 1. Time-Off Coordination. Al Assistant seamlessiy communicates with the relevant HR management systems to record the necessary employees' details. 2. Recruitment Automation. Al Assistant interacts with hiring platforms to create and distribute job listings matching company needs. 3. Visual Content Generation. Using innovative algorithms, Al Assistant generates images for marketing and team resources. 4. Financial Analysis. Al Assistant retrieves and analyzes detailed financial data, allowing for in-depth analysis and strategic planning. 5. Software Development Support. Integrated with code management tools, Al Assistant supports coding and guality assurance. Ol... 07 08 09 10 1 12 13 1"}
{"unique_id": "581f381b-9b2c-497d-8280-c689d7ec1f75", "file_name": "How to Build a Knowledge Assistant at Scale.docx", "file_type": ".docx", "chunk_id": 0, "chunk_text": "Skip to content MLOps Community Join Learn Tools Blog Events Videos Partner December 22, 2023 How to Build a Knowledge Assistant at Scale QuantumBlack Team Jannik Wiedenhaupt, Roman Drapeko, Mohamed Abusaid, Nayur Khan QuantumBlack, AI by McKinsey unlocks the power of artificial intelligence to help organizations blend AI and cutting-edge solutions with strategic thinking and domain expertise. www.mckinsey.com/capabilities/quantumblack/ Introduction The discussion about the myriad applications of Large Language Models (LLMs) is extensive and well-trodden in tech circles1. These models have opened many use cases, reshaping sectors from customer service to content creation. However, an often-overlooked aspect in this discourse is the practicality of productizing and scaling these use cases to support tens of thousands of users2. The challenges range from expanding server capacity, tweaking algorithms, ensuring robustness and reliability, and maintaining privacy and security. In this article, we describe some of the considerations necessary when developing an enterprise-level knowledge assistant (KA) and introduce a scalable architecture. Foundational Architecture Principles A well-designed KA must offer round-the-clock operation in a demanding enterprise environment and embody more than just cutting-edge AI capabilities. It should be fine-tuned for quality and speed and structured for continuous improvement, allowing for seamless integration and evolution of new functionalities. These operational imperatives set the stage for this proposed architectural design. To achieve these high standards of operational excellence, the KA is built upon five foundational architecture principles. Each principle plays a critical role in ensuring that the KA meets the immediate needs of a large user base and remains a versatile and forward-looking solution, ready to adapt and grow with the changing landscape of enterprise requirements. Scalability: Addressing the high volume of interactions and the expansion needs of AI applications. Security: Ensuring data safety and strict access control in a world where information security is paramount. Transparency: Offering clear insights into system operations, usage metrics, and cost implications. Modularity: Facilitating easy upgrades and modifications to stay abreast of technological advancements. Reusability: Promoting efficiency and collaboration by designing components that can be used across various projects. These foundational architecture principles are intricately woven into every aspect of the suggested design, forming the backbone of a retrieval-augmented generation (RAG) architecture. Early-Stage Decisions Enhancing KA’s Foundational Principles Quality Over Cost: We know quality matters. This foundational choice means accepting more significant upfront expenses linked to token usage and infrastructure. This decision is worthwhile as better performance and reliability from these quality investments bring tangible savings. Service-Based LLMs: Another critical early decision adopts a service-based approach to LLMs. This choice underscores the need for flexibility and scalability in a KA’s language-processing capabilities. By integrating state-of-the-art service-based LLMs, any KA is equipped to rapidly adapt to changing conditions and technological advances, positioning it as a cutting-edge solution in this technology realm. LLM-Agnosticism: As the space of generative AI develops, and new players and models enter the space regularly, it is essential that a KA is future-proofed by offering the option to switch the underlying LLM(s) easily. These early-stage decisions shape the design of"}
{"unique_id": "581f381b-9b2c-497d-8280-c689d7ec1f75", "file_name": "How to Build a Knowledge Assistant at Scale.docx", "file_type": ".docx", "chunk_id": 1, "chunk_text": "as a cutting-edge solution in this technology realm. LLM-Agnosticism: As the space of generative AI develops, and new players and models enter the space regularly, it is essential that a KA is future-proofed by offering the option to switch the underlying LLM(s) easily. These early-stage decisions shape the design of a KA into a robust, adaptable, and high-performing enterprise KA. As we explore the multi-layered architecture of such a KA in the following sections, we’ll see how these enhanced principles drive the design and functionality of each layer in the system. A RAG architecture At the core of the KA is a carefully crafted architecture segmented into four essential layers, each with its unique function and set of challenges. This multi-layered approach forms the KA’s structural and operational framework, grounded by the foundational principles described above. Data Layer: The foundation, where vast amounts of data are processed and prepared for retrieval. It is crucial for the KA’s enterprise-specific intelligence. LLM Layer: The general-purpose intelligence and processing center for all language model requests, ensuring contextually accurate and relevant responses. Reporting Layer: The analytical segment, which provides usage, cost, and performance metrics insights. Application Layer: The user-facing interface and backend with business logic – a key layer of the KA that navigates logic for forming responses to end-users. As we embark on a detailed journey through the layers, we will briefly examine the Data, LLM, and Reporting layers, highlighting their roles and significance. The spotlight will then shift to an in-depth look at the Application Layer, where the KA’s functionalities come to life, directly interacting with, and serving, the end-users. 1. Data Layer The Data Layer of the KA is integral to its “enterprise-specific” intelligence, anchored by a vector store that stores documents in chunks, along with their embeddings and metadata4. This vector store is essential for facilitating search using semantic similarity on a large scale, ensuring performance remains robust even as data volumes expand. LLMs have limits on how much data they can accept and process at one time (also known as token limits), making it hard to process long documents simultaneously. We recommend use of a well-known “chunking” technique to break documents into smaller parts to solve this. This enables search across the whole document in steps, avoiding an LLM’s token limit. Metadata stored alongside chunks enables us to associate information found during searches with source documents. Custom pipelines enrich the data with as much relevant metadata as possible to improve search results. This capability maintains context and relevance in the KA’s responses. Selecting the suitable vector database and the appropriate chunking strategy is critical. Different chunking strategies, such as syntactic versus semantic, variable versus fixed, play distinct roles in how data is processed and retrieved5. To handle the vast amounts of data, we recommend a Data Lake with data processing pipelines, implemented with a framework like the open-source Python-based Kedro6. These pipelines should be tasked with parsing, chunking, metadata enrichment, and vectorizing data chunks, subsequently populating the vector databases."}
{"unique_id": "581f381b-9b2c-497d-8280-c689d7ec1f75", "file_name": "How to Build a Knowledge Assistant at Scale.docx", "file_type": ".docx", "chunk_id": 2, "chunk_text": "how data is processed and retrieved5. To handle the vast amounts of data, we recommend a Data Lake with data processing pipelines, implemented with a framework like the open-source Python-based Kedro6. These pipelines should be tasked with parsing, chunking, metadata enrichment, and vectorizing data chunks, subsequently populating the vector databases. These pipelines need well-structured and indexed data storage, so it is crucial to have healthy data quality and governance in place. Additionally, the Data Layer can provide access to various knowledge APIs, like a People Directory and a Wiki, to further enrich the KA’s responses. These APIs may offer additional context and relevant information, enhancing the capability to deliver tailored and intelligent responses. Finally, it’s important to control who can access what. If a user can’t see a document, the KA should not take it into response formation. The data access control component should be decoupled from the KA itself. This approach not only fortifies security and ensures compliance but also elegantly paves the way for seamless scalability across multiple KAs. 2. LLM Layer The LLM Layer in the KA’s architecture serves as the central unit of processing. This layer is uniquely designed to handle the complexities and demands of processing language model requests, playing a critical role in the functionality of the KA. A key component of the LLM Layer is the LLM API Gateway. This gateway is the conduit through which all requests pass, acting as a centralized processing point. Its design includes scalable-on-demand integrations with multiple LLM vendors, offering the flexibility to easily switch services as needed. This versatility is crucial in maintaining operational efficiency and adapting to various requirements or changes in vendor capabilities. An important function of the LLM API Gateway is its ability to track the costs associated with using LLMs (e.g. tokens generated, subscriptions). This feature is vital for managing the operational budget and optimizing resource allocation. Additionally, the gateway logs all interactions in a logging platform. This logging is not just about keeping a record; it’s a treasure trove of data that can be analyzed for improvements, troubleshooting, and understanding usage patterns. Within this layer, there is direct access to both LLM models and Embedding models. The LLM models are the backbone of the KA’s language understanding and generation capabilities. Meanwhile, the Embeddings models, which are also used by the Data Layer for vectorizing document chunks, play a critical role in enhancing the semantic search capabilities of the KA. 3. Reporting Layer The Reporting Layer in any KA’s architecture is essential for providing transparency on several critical fronts: costs, usage, and data analytics. This layer is intricately designed to capture and present a comprehensive view of the KA’s operational dynamics, making it an invaluable tool for both management and continuous improvement. One of the primary functions of the Reporting Layer is cost analysis to track and analyze all expenses related to the operations of the KA. This includes costs associated with token consumption by LLMs, data processing, and other computational resources. By"}
{"unique_id": "581f381b-9b2c-497d-8280-c689d7ec1f75", "file_name": "How to Build a Knowledge Assistant at Scale.docx", "file_type": ".docx", "chunk_id": 3, "chunk_text": "an invaluable tool for both management and continuous improvement. One of the primary functions of the Reporting Layer is cost analysis to track and analyze all expenses related to the operations of the KA. This includes costs associated with token consumption by LLMs, data processing, and other computational resources. By offering detailed insights into these expenditures, the Reporting Layer enables effective budget management and helps identify opportunities for cost optimization. Another crucial aspect of this layer is usage monitoring. It keeps a close watch on how the KA is being used across the organization. This monitoring covers various metrics, such as the number of user interactions, peak usage times, and the types of queries being processed. Understanding these usage patterns is vital for scaling the KA effectively and ensuring it meets the evolving needs of the enterprise. Additionally, the Reporting Layer delves into data analytics, providing an in-depth look at the performance and effectiveness of the KA. This includes analyzing response accuracy, user satisfaction, and the overall efficiency of the KA’s operations. Such analytics are instrumental in guiding future improvements, ensuring the KA remains a cutting-edge tool for the enterprise. 4. Application Layer The Application Layer is where the functionality of the KA comes to the forefront, directly engaging with users. This layer is where user queries are generated, received, processed, and responded to, encompassing the end-to-end interaction that defines the user experience. The Application Layer comprises of four main components: Frontend: This is the user interface of the KA, where users interact and input their queries. Operational Stores: These are databases that store the KA’s conversational history and user feedback. Configuration Stores: This component contains glossaries for query improvement and prompts from response generation. Backend: The backend processes API requests from frontend, handling the intricate task of understanding and generating responses integrating to services from LLMs and Data Layers Frontend The frontend of the KA should be a straightforward web interface, typically crafted using React and JavaScript. Design should consider ease of use, for users to simply ask questions, receive answers, and access guidelines for effective interaction with the KA. This interface design may consider inclusion of a feature for users to provide feedback, essential for refining the KA’s performance. Responses to user queries should be supported by clearly cited sources to offer a reliable reference for the shared information. Additionally, answers may include links to relevant enterprise micro-sites or suggest contacts within the organization who can offer further assistance on the topic. This approach adds a layer of practical utility to each response, directing users to additional resources or personnel that can provide more in-depth support or information. The modular design of the KA architecture plays a key role here. It allows for the possibility of substituting a frontend with alternative interfaces in the future, such as a mobile app or an instant messaging platform. This flexibility comes about because the backend interactions occur through APIs, enabling seamless integration with various frontends while maintaining consistent functionality and user"}
{"unique_id": "581f381b-9b2c-497d-8280-c689d7ec1f75", "file_name": "How to Build a Knowledge Assistant at Scale.docx", "file_type": ".docx", "chunk_id": 4, "chunk_text": "role here. It allows for the possibility of substituting a frontend with alternative interfaces in the future, such as a mobile app or an instant messaging platform. This flexibility comes about because the backend interactions occur through APIs, enabling seamless integration with various frontends while maintaining consistent functionality and user experience. Operational Stores Operational stores form the backend persistence layer, responsible for the storage of conversation history, user settings, feedback, and other critical operational data essential for the KA to be functional. Conversation history is particularly important for providing historic context to the LLM, enhancing the relevance of responses in ongoing interactions. Additionally, the information gathered in operational stores is crucial for the continuous improvement of the KA. This data is analyzed within the Data Layer to identify trends and areas for enhancement, directly influencing the KA’s development and refinement. Insights derived from this analysis are then presented in the Reporting Layer, providing a comprehensive view of the KA’s interactions and effectiveness, which is vital for its ongoing optimization and success. Backend The backend is where the core business logic of the KA resides. It’s structured as a set of components, each with a single responsibility, working together to process user interactions efficiently. At a high-level, it is an orchestration of different decisions and LLM operations. It handles critical functions such as accessing the Data Layer and LLMs, analyzing incoming requests, formulating messages, and delivering responses. Each component is designed to perform its specific task effectively, ensuring that the entire process from query intake to response delivery is smooth and precise. The following section traces a user query through the complete backend architecture. Input Handler Request Handler The Application Layer of the KA activates upon receiving a user query through an API. The Request Handler manages chat interactions and retrieves the last few messages in a conversation from the Conversation History Store. Additionally, the Request Handler loads the current configurations for the LLMs used in the application. Input Guardrails Once the necessary database operations are completed, the Input Guardrails apply. In any specific context, input guardrails encompass a selection of policies, business rules, and validations, and are designed to ensure that incoming requests meet predefined criteria and may proceed. The primary objective of these guardrails is to prevent users from using the system in ways that deviate from its intended purpose. For instance, in the scenario of a flight booking app, customers should not have the capability to inquire about other passengers beyond the scope of their valid booking. Guardrails are essentially a stack of functions arranged in a predetermined order. Each function evaluates the incoming request input and its metadata and takes one of three possible actions. The possible actions include “pass” indicating that the guardrail approves the request without any issues; “update” this suggests the request requires modification before being allowed to pass; and “reject” signaling that the request failed the guardrail and cannot continue for processing, this terminates the process and returns a rejection reason to the"}
{"unique_id": "581f381b-9b2c-497d-8280-c689d7ec1f75", "file_name": "How to Build a Knowledge Assistant at Scale.docx", "file_type": ".docx", "chunk_id": 5, "chunk_text": "actions include “pass” indicating that the guardrail approves the request without any issues; “update” this suggests the request requires modification before being allowed to pass; and “reject” signaling that the request failed the guardrail and cannot continue for processing, this terminates the process and returns a rejection reason to the requester. This approach ensures that requests that fail the guardrails are rejected early and if requests require modifications before being shared further then this is handled appropriately, this not only ensures adherences to intended use cases but also efficiently processes incoming requests for maximum reliability. One such update-guardrail is the Query Improver. This component is crucial for adapting domain-specific terminology to enhance later retrieval processes. In many industries and business, queries include niche jargon, abbreviations, and phrases unique to the industry. These can be obscure or have different meanings in general language. To address this, the implementation should include a comprehensive glossary of company-specific terms and abbreviations. This glossary “translates” and modifies user queries for optimal retrieval. For instance, it could remove trailing punctuation and expand acronyms in the queries (e.g. “MVP” is reformulated as “MVP (Minimum Viable Product)”). Such alterations significantly boost the retrieval effectiveness on proprietary data. Eliminating punctuation aids in aligning the query’s semantic similarity with a corpus, which predominantly consists of statements rather than questions. Expanding abbreviations is doubly beneficial: it increases the prominence of key terms in the retrieval process, ensures coverage of content that may only use the expanded form, and aids the chat model in accurately interpreting the user’s intent. Such refinements are instrumental in enhancing the overall performance and accuracy of a KA. The next step in the process is the Intent Recognition module, a common feature in LLM applications designed to bring structure to the typically unstructured nature of LLMs. This module’s function is to categorize each user query into one of several pre-defined intents. The identified intent plays a dual role: it guides the subsequent control flow within the application and enhances the effectiveness of the knowledge retrieval system. The most reliable method for intent recognition isn’t a highly specialized machine learning model but rather an LLM. To improve the LLM’s accuracy, we suggest a few-shot prompting technique with balanced examples for each intent. For instance, if we have five intents and three examples per intent to ensure accurate classification, then every intent is represented with three examples. This gives us a total of 15 examples in the prompt. This method is highly effective for setups with fewer than ten intents, achieving over 90% accuracy. However, it’s important to note that this approach has its limitations. As the number of intents increases, adding more examples becomes less practical, and distinguishing between intents becomes more challenging. Response Formation Data Source Routing The Data Source Routing module determines where the KA receives its knowledge based on the user’s intent. With the user’s intent, the KA picks between three primary data sources, each accessible through a custom search algorithm or external APIs: Vector"}
{"unique_id": "581f381b-9b2c-497d-8280-c689d7ec1f75", "file_name": "How to Build a Knowledge Assistant at Scale.docx", "file_type": ".docx", "chunk_id": 6, "chunk_text": "between intents becomes more challenging. Response Formation Data Source Routing The Data Source Routing module determines where the KA receives its knowledge based on the user’s intent. With the user’s intent, the KA picks between three primary data sources, each accessible through a custom search algorithm or external APIs: Vector Store: Text documents, like PDFs, PowerPoints, and Word documents. All chunks have metadata that enables filtering like title, abstract, authors etc. People Directory API: Personnel information, specific skills, and contact details. Internal Wiki API: Company-related information, IT instructions, HR documents and more. The real advantage of intent recognition lies in its flexibility to incorporate additional data sources as needed. Beyond enhancing the control over the KA’s outputs, selective querying of data sources offers another significant benefit. While many solutions emphasize vector stores and semantic similarity search, not all data types are equally suited for such methods. For example, a people directory, with its distinct data, doesn’t fit as seamlessly into an embedding database as long documents do. In a standard similarity search, even well-detailed people profiles might not rank high enough to be included in the top results. Intent recognition circumvents this issue through clearly defined control flow. This behavior can be implemented using different chains for different intents, as in the example below. INTENT_CHAIN_MAPPING = { IntentType.KNOWLEDGE: KnowledgeChain, IntentType.PEOPLE: PeopleChain, IntentType.SUPPORT: WikiChain, IntentType.CHAT: ChatChain, IntentType.DOMAIN_KNOWLEDGE: DomainKnowledgeChain, } def get_response(question: str, conversation_history: List[Message]): chain = intent_chain_mapping[intent] llm = AIGateway(model=’gpt-4’) llm_task = asyncio.create_task(chain.acall(question, llm, conversation_history)) # ... response = await llm_task return response ### ---------------- ### class KnowledgeChain(Chain): # ... class DomainKnowledgeChain(Chain): # ... class ChatChain(Chain): # ... class PeopleChain(Chain): # ... Retriever The question is then passed to the Retriever. Depending on the targeted search the retriever will either embed the question through LLM Gateway and perform a semantic similarity search, use it for a keyword search, or pass it to an external API that handles the retrieval. The Retriever should be tailored to manage different types of data effectively. Each data source not only varies in content but also in the optimal amount of information to retrieve. For example, the breadth and depth of data needed from a people directory differs significantly from the same required from a knowledge base. To address this, the retrieval logic needs customization. For people-related queries, the retriever is configured to return a concise list of the top five most relevant contacts from the directory. In contrast, a search for knowledge yields a broader set, pulling up to 20 chunks of information to provide a more comprehensive context. This approach, however, is not rigid. For specific intents where a more integrated perspective is beneficial, a Retriever should combine data from multiple sources. For instance, a user seeking guidance in a specific domain will receive information both from the vector store and the wiki as a mix is likely to be most useful to the user. Fine-tuning the process means definition of a search algorithm that can combine different semantic similarity search algorithms, exact keyword"}
{"unique_id": "581f381b-9b2c-497d-8280-c689d7ec1f75", "file_name": "How to Build a Knowledge Assistant at Scale.docx", "file_type": ".docx", "chunk_id": 7, "chunk_text": "a user seeking guidance in a specific domain will receive information both from the vector store and the wiki as a mix is likely to be most useful to the user. Fine-tuning the process means definition of a search algorithm that can combine different semantic similarity search algorithms, exact keyword matching and metadata filtering. Additionally, it may require manual tuning in how many items to retrieve from each source for each intent. Continuous feedback loops with early-access users are crucial in the optimization process, to iteratively refine the retrieval strategies until the balance of quantity, quality, and source diversity is just right. Context Enrichment A KA’s context enrichment phase requires crafting effective prompts. These prompts must harmoniously blend instructions from the assistant, retrieved context, and the chat history. This process is heavily influenced by the detected intent and come with varying levels of difficulty in consolidating data from different sources. A significant challenge may typically arise in ensuring the relevance and conceptual cohesion for queries seeking pure knowledge. To mitigate reliance on semantic search and enhance accuracy of the final chat completion, there should be consideration given to the strategy inspired by the MapReduceChain in LangChain7 (see KnowledgeChain example below). This method involves deploying parallel LLM calls for each information chunk, instructing the model to not just evaluate but also synthesize information across these chunks. This approach is pivotal in ensuring that source citations are accurate. Instead of depending on the LLM to reproduce source links – a method prone to inaccuracies – you should embed source referencing directly into code logic. Furthermore, you should integrate recent conversation history into this enrichment process, enhancing the KA’s ability to provide contextually relevant responses. One strategy uses a common buffer window approach, focusing on the last 3-5 exchanges. The approach not only ensures relevance and continuity in conversations but also conserves tokens, proving more efficient than longer memory spans or more complex methodologies. class KnowledgeChain(Chain): # ... async def acall( self, question: str, llm: LanguageModel, conversation_history: List[Message], retrieve_k: int = 20, filter_k: int = 5, ): documents = self.retrieve(question, k=retrieve_k) filtered_documents = self.filter(documents, k=filter_k) # Map step llm_response = self.answer(filtered_documents, question, conversation_history) # Reduce step answer = self.postprocess(llm_response, filtered_documents) return answer KA Response After all the steps above to collect relevant information, the next step answers the user’s question using an LLM. As a reminder, the LLM prompt needs to include data from the databases (see Data Source Routing and Retriever) and conversation history. Additionally, it’s necessary to give clear instructions to the LLM and format the input data correctly, to determine its behavior, tone, and adherence to the given data. Precise prompt engineering becomes a real challenge, as the outputs need to be accurate and reliable for critical decision-making. The diversity of topics, spanning hundreds of complex subjects, presents another layer of complexity. To ensure the quality and relevance of responses, there should be a set of early users to test the experience, and subject matter experts from various fields to"}
{"unique_id": "581f381b-9b2c-497d-8280-c689d7ec1f75", "file_name": "How to Build a Knowledge Assistant at Scale.docx", "file_type": ".docx", "chunk_id": 8, "chunk_text": "to be accurate and reliable for critical decision-making. The diversity of topics, spanning hundreds of complex subjects, presents another layer of complexity. To ensure the quality and relevance of responses, there should be a set of early users to test the experience, and subject matter experts from various fields to test the correctness of the KA’s responses. Their insights will be invaluable in refining the KA’s instructions and guiding the selection of source documents. Output Handler Output Guardrails After receiving the final chat completion from the LLM, the next step post processes it in the Output Handler. It is typical to find that no matter how carefully you engineer a prompt and steps in advance of the model, there always remains a final risk of hallucinations, and undesirable information being shown to users8. To mitigate this risk, there should be a set of Output Guardrails in place. These are a set of asynchronously executed checks on the model’s response that include a content filter and a hallucination detector. The content filter detects and removes biased and harmful language as well as removing any personal identifiable information (PII). The hallucination detector checks whether there is any information in the response that is not given in the retrieved context. Both guardrails are based on LLMs. Besides mitigating risk, they also inform future development and troubleshooting efforts. Response Handler Afterward, if the chat completion passes the output guardrails, the final step formats the response, and sends it to the front end for the user to read. Summary of Considerations We summarize some of the considerations covered earlier in this article. Chunking: The selection of a chunking strategy significantly impacts the performance of semantic similarity search, the use of context, and the understanding of specific knowledge topics by the language model. Guardrails: Implementing guardrails for input/output is crucial to mitigate risks and ensure the reputation of AI applications in enterprise settings. These guardrails can be customized and developed according to the organization’s risk requirements. Configuration Database: Maintaining a database table to track LLM configurations allows for efficient monitoring, potential rollback capabilities, and the association of specific model versions with user feedback and errors. Search: Fine-tuning the search algorithm involves combining semantic similarity search algorithms, exact keyword matching, and metadata filtering, while continuously optimizing retrieval strategies based on user feedback to achieve the right balance of quantity, quality, and source diversity. Prompt Engineering: Effective prompting is key to the success of an application and can be collaboratively done with users and/or experts. Controlling LLMs: Introducing intent recognition or a similar deterministic split enhances control flow and provides developers with more control over the behavior of LLM applications. Making Data LLM-ready: Cleaning unstructured data from artifacts (e.g., footers in the middle of chunks) and adding relevant metadata (e.g., titles) to chunks allows LLMs to effectively understand different data types. Separating Data Sources: While it may be tempting to mix all types of data in a vector store and use semantic similarity search, different data types have"}
{"unique_id": "581f381b-9b2c-497d-8280-c689d7ec1f75", "file_name": "How to Build a Knowledge Assistant at Scale.docx", "file_type": ".docx", "chunk_id": 9, "chunk_text": "(e.g., footers in the middle of chunks) and adding relevant metadata (e.g., titles) to chunks allows LLMs to effectively understand different data types. Separating Data Sources: While it may be tempting to mix all types of data in a vector store and use semantic similarity search, different data types have different requirements, and querying them separately yields much better results. Domain Knowledge: Incorporating specific knowledge through glossaries, prompt engineering, or fine-tuning is essential for LLMs to understand industry or company-specific knowledge. Conclusion In the realm of corporate technology, the integration and application of LLMs offer intriguing insights into the evolving landscape of data management, system architecture, and organizational transformation. This article aims to shed light on these aspects, with an emphasis on the broader application of LLMs within corporate settings. Our discussion is just the beginning of a deeper exploration into the various layers, including data handling, LLM optimization, and impact assessment, essential for deploying advanced LLM applications. Future articles will delve into the general infrastructure requirements and best practices for implementing LLMs in a corporate environment, along with exploring diverse AI use cases. For those interested in the expanding field of LLMs and their scalable applications, we invite suggestions on topics of interest. Don’t forget to subscribe to the MLOps Community Newsletter to ensure you don’t miss our upcoming content. Footnotes: 1. https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/the-economic-potential-of-generative-ai-the-next-productivity-frontier 2. https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/what-every-ceo-should-know-about-generative-ai 3. https://arxiv.org/abs/2005.11401 4. https://vercel.com/guides/vector-databases 5. https://www.pinecone.io/learn/chunking-strategies/ 6. https://kedro.org/ 7. https://python.langchain.com/docs/modules/chains/document/map_reduce 8. https://www.nytimes.com/2023/07/27/business/ai-chatgpt-safety-research.html Authors QuantumBlack Team Jannik Wiedenhaupt, Roman Drapeko, Mohamed Abusaid, Nayur Khan QuantumBlack, AI by McKinsey unlocks the power of artificial intelligence to help organizations blend AI and cutting-edge solutions with strategic thinking and domain expertise. View all posts Jannik Wiedenhaupt Data Scientist at McKinsey || M.S. Columbia || CDTM || TU Munich View all posts Roman Drapeko Distinguished Data Engineer (Snr. Director) at QuantumBlack, AI by McKinsey View all posts Mohamed Abusaid Associate Partner at QuantumBlack, McKinsey & Co. View all posts Nayur Khan Partner - QuantumBlack, AI by McKinsey🔹DataIQ 100 - 2023🔸Keynote Speaker🔹Scaling AI🔸D&I Lead🔹MLOps🔸Responsible AI🔹Software Engineering View all posts Related posts: Empowering Language Model Applications: Understanding and Evaluating Vector Databases in Production 🤸⚕️Unleashing the Power of Large Language Models in Healthcare and Wellness: Practical Context Providing in Healthcare and Wellness with Mistral 7B Building Neoway’s ML Platform with a Team-First Approach and Product Thinking What I Learned Building Platforms at Stitch Fix Vector Similarity Search: From Basics to Production Tags: Knowledge Assistant, LLMs, MLops Privacy Policy ©2025 MLOps Community. All rights reserved unless states. Images provided by Unsplash.com and pexels.com.Made with ♥, tea and biscuits. Join Learn Tools Blog Events Videos Partner Slack Youtube Medium Twitter Linkedin"}
{"unique_id": "63d517bf-4b8f-4adf-bc00-a3fbf9aa49fe", "file_name": "CrateDB Blog _ Core Techniques Powering Enterprise Knowledge Assistants.html", "file_type": ".html", "chunk_id": 0, "chunk_text": "CrateDB Blog | Core Techniques Powering Enterprise Knowledge Assistants Live Stream on Jan 23rd: Unlocking Real Time Insights in the Renewable Energy Sector with CrateDB Register now Skip to content Product Database Overview CrateDB Cloud CrateDB Self-Managed SQL examples Integrations Security Data models Time-series Document/JSON Vector Full-text Spatial Relational Use cases AI/ML integration AI-powered chatbots Internet of Things Digital twins Geospatial analytics Log & event analysis Database consolidation Industries Energy Financial Services FMCG Logistics Manufacturing Oil, gas & mining Smart city solutions Technology platforms Telco Transportation Resources Customer stories Academy Asset library Blog Events Developer Documentation Drivers and tools Community GitHub Support Pricing Log In Start free Log In Start free Blog Core Techniques Powering Enterprise Knowledge Assistants 2025-01-15 by Wierd van der Haar , 5 minute read chatbot To harness the potential of RAG, organizations need to master a few crucial building blocks. *** This article is part of blog series. If you haven't read the previous article yet, be sure to check it out: Building AI Knowledge Assistants for Enterprise PDFs: A Strategic Approach 1. Extracting from PDFs Before you can feed your data into an RAG pipeline, you need to extract it from PDFs. This step sets the foundation for the entire workflow. The goal of your chatbot—whether it needs to present actual images, provide text-only responses, or generate image descriptions—directly impacts how you extract and process each PDF. For instance, if your chatbot must display or summarize images, you’ll need dedicated mechanisms to handle, store, and retrieve them; if you’re only interested in text, you can focus on raw text extraction and OCR. Text Extraction Use libraries or services that identify text within PDFs. For straightforward text, standard PDF parsing libraries work. However, be mindful of formatting, especially in scanned PDFs with no digital text layer. Also consider headers and footers, which often contain valuable information like document titles, chapter names, page numbers, or dates. You may opt to remove them from the main body of text and store them separately as part of the document’s metadata. Image Detection Some PDFs include images or diagrams that may hold critical information. Identifying these images is essential if you need a fully comprehensive pipeline that can reference not just text but also visual elements. OCR (Optical Character Recognition) OCR transforms the scanned images of text into machine-readable text, thereby creating a “digital text layer” where none existed before. This ensures you can index, extract, and analyze the content just like any other text-based PDF. The process can be resource-intensive (often requiring GPUs), but it’s indispensable for processing large volumes of scanned documents. Table Extraction When PDFs contain data in tabular format, consider using specialized tools or libraries (e.g., Tabula, Camelot) to extract tables accurately. Tables often include important figures or text organized in rows and columns, which may otherwise be lost if parsed as standard text. Decide whether to keep the table structure (e.g., converting to CSV or HTML) or to summarize the data for downstream tasks like"}
{"unique_id": "63d517bf-4b8f-4adf-bc00-a3fbf9aa49fe", "file_name": "CrateDB Blog _ Core Techniques Powering Enterprise Knowledge Assistants.html", "file_type": ".html", "chunk_id": 1, "chunk_text": "Tabula, Camelot) to extract tables accurately. Tables often include important figures or text organized in rows and columns, which may otherwise be lost if parsed as standard text. Decide whether to keep the table structure (e.g., converting to CSV or HTML) or to summarize the data for downstream tasks like embedding or semantic search. Metadata Collection Don’t forget about titles, authors, creation dates, and other metadata. These details help with advanced filtering and can also influence the retrieval steps later. In some cases, you might add header and footer data here if it provides contextual clues or helps distinguish versions of a document. 2. Chunking Extracted Data Unlike plain search indexes, RAG pipelines often split documents into chunks—manageable text segments used to create embeddings. The reason is simple: LLMs work better when prompts are concise, context-rich, and specific. Fixed-Size Chunking (with overlap): Straightforward approach where each chunk is a fixed number of tokens or characters, and adjacent chunks overlap slightly to retain context. Structure and Content-Aware Chunking: Considers sentences, paragraphs, sections, or chapters when chunking. Preserve logical boundaries, which can significantly improve retrieval quality. Document-Based Chunking: With this chunking method, you split a document based on its inherent structure. This approach respects the natural flow of the content but may not be as effective for documents that lack a clear structure. Hierarchical Chunking: Combines fixed-size and structure-awareness by chunking at multiple levels (document → chapter → paragraph) and linking them in a parent-child relationship. Semantic Chunking: The main idea is to group text segments with similar meaning. You create embeddings for each segment, then compare those embeddings to see which ones are most closely related. This approach keeps similar ideas together, preventing arbitrary splits that could harm retrieval quality. Agentic Chunking: This method empowers an LLM to dynamically decide how to split the text into chunks. We begin by extracting short, independent statements from the text and let an LLM agent determine if each statement should join an existing chunk or start a new one. Because the model understands context, it can produce more coherent chunks than fixed or structural methods. 3. Generating Embeddings Once you have your chunks, each chunk needs a vector representation (an embedding) that captures its semantic meaning. Choosing Embedding Models Security Considerations: The classification of your documents might prohibit the use of online LLMs, pushing you toward an on-premise or self-hosted model. If confidentiality is a priority, local deployment can ensure that no data leaves your environment. Data Types (Text, Tables, Images, Multimodal): Text: A text-based embedding model may be sufficient if you only have textual data. Tables: For tabular data, you may need to transform the table into a more descriptive text format or use a specialized approach to preserve row/column relationships. One strategy is to summarize tables first and then generate embeddings from those summaries. Images: If your chatbot must search for images via text or by providing another image (“show me images similar to this”), you’ll need to generate embeddings for the"}
{"unique_id": "63d517bf-4b8f-4adf-bc00-a3fbf9aa49fe", "file_name": "CrateDB Blog _ Core Techniques Powering Enterprise Knowledge Assistants.html", "file_type": ".html", "chunk_id": 2, "chunk_text": "use a specialized approach to preserve row/column relationships. One strategy is to summarize tables first and then generate embeddings from those summaries. Images: If your chatbot must search for images via text or by providing another image (“show me images similar to this”), you’ll need to generate embeddings for the images If you only need to display the original images without advanced search features, you may opt to store them directly in your database. Multimodal models (e.g., CLIP or GPT-4 Vision) can handle both text and images, enabling semantic search across different data types. Task Orientation: Think about the end goal—text-to-text, image-to-text, image-to-image, or table-based queries. Different scenarios benefit from specialized embedding models. Performance Considerations Hardware Requirements: Embedding models often require GPUs or other accelerators for efficient batch processing. Local vs. Cloud Deployment: Weigh the cost and convenience of a cloud solution against the benefits of total control and data sovereignty offered by an on-premises model. Image & Table Processing: Generating embeddings for images or large tables typically requires more compute resources and sometimes specialized libraries or frameworks. 4. Storing the Data The diversity of data and the sophistication of AI models demand a flexible, powerful, and nuanced approach to data management. As AI continues to penetrate various sectors, the need for databases that can adapt to complex data landscapes becomes paramount. The future of multi-model databases in AI shines—as an enabler of complex, context-rich, and real-time intelligent applications. In a RAG workflow, you need to store: Raw Text (and possibly images or OCR’d text) Embeddings (vectors) Metadata (title, author, date, source) A robust multi-model database that can handle real-time ingestion of large datasets, manage high concurrency, and scale horizontally is a key piece of infrastructure. It should offer flexibility (for structured, unstructured, or semi-structured data), speed (sub-second queries on large datasets), and advanced search functionalities. *** Continue reading: Designing the Consumption Layer for Enterprise Knowledge Assistants Share Related Posts Building AI Knowledge Assistants for Enterprise PDFs: A Strategic Approach 2025-01-15 In today’s increasingly data-driven world, many organizations are sitting on mountains of information locked away in PDFs. Whether it’s business reports, regulatory documents, user manuals, or researc... Read more Step by Step Guide to Building a PDF Knowledge Assistant 2025-01-15 This guide outlines how to build a PDF Knowledge Assistant, covering: Setting up a project folder. Installing dependencies. Using two Python scripts (one for extracting data from PDFs, and one for cr... Read more Designing the Consumption Layer for Enterprise Knowledge Assistants 2025-01-15 Once your documents are processed (text is chunked, embedded, and stored) — read \"Core techniques in an Enterprise Knowledge Assistant\" — , you’re ready to answer user queries in real time. This stage... Read more Follow us on Twitter Follow us on Twitter Follow us on GitHub Follow us on GitHub Follow us on YouTube Follow us on YouTube Follow us on GitHub Follow us on GitHub Company Leadership Team Investors Career Events Newsroom Media kit Ecosystem Partners Startups Integrations Contact Contact us Offices Security Support"}
{"unique_id": "63d517bf-4b8f-4adf-bc00-a3fbf9aa49fe", "file_name": "CrateDB Blog _ Core Techniques Powering Enterprise Knowledge Assistants.html", "file_type": ".html", "chunk_id": 3, "chunk_text": "us on Twitter Follow us on Twitter Follow us on GitHub Follow us on GitHub Follow us on YouTube Follow us on YouTube Follow us on GitHub Follow us on GitHub Company Leadership Team Investors Career Events Newsroom Media kit Ecosystem Partners Startups Integrations Contact Contact us Offices Security Support © 2024 CrateDB. All rights reserved. | Legal | Privacy Policy | Imprint"}
